#################
# Train/Test    #
#################

world: 
  type: 'closed'  # 'closed' or 'open'
  observed_fraction: 0.20  # fraction of the open world that can be measured by the adversary 
num_kfolds: 10



######################
# Preprocessing      #
######################

feature_scaling: True  # Rescale each feature to mean zero and unit standard deviation



#################
# Classifiers   #
#################

# All supported models 
# model: ['RandomForest', 'RandomForestBagging', 'RandomForestBoosting', 'ExtraTrees',
#         'AdaBoost', 'LogisticRegression', 'SVM', 'GradientBoostingClassifier',
#         'DecisionTreeClassifier', 'SGDClassifier', 'KNeighborsClassifier']

models: ['RandomForest', 'ExtraTrees', 'DecisionTreeClassifier', 'RandomForestBagging',
         'RandomForestBoosting', 'Wa-kNN']
parameters:
  Wa-kNN:
    rounds: [1, 10, 50, 100, 200]
    n_neighbors: [1, 3, 5, 10, 25, 50, 100] # Higher values trade TPR for FPR
    reco_points_num: [1, 3, 5, 10, 25, 50, 100]
  RandomForest:
    n_estimators: [25, 50, 100] #[25, 50] # 100, 1000, 10000, 10
    max_depth: [10, 20, 50, 100] # 50, 100, 5
    max_features: ['sqrt', 'log2', 2, 4, 8, 16, "auto"]
    criterion: ['gini', 'entropy']
    min_samples_split: [2,  5, 10]
  RandomForestBagging:
    n_estimators: [10] # [25, 50, 100, 1000, 10000]
    max_depth: [5] # [10, 20, 50, 100]
    max_features: ['sqrt'] # ['log2', 2, 4, 8, 16, "auto"]
    criterion: ['gini'] # ['entropy']
    min_samples_split: [2] # [5, 10]
    max_samples: [0.5] # [1.0]
    bootstrap: [True]
    bootstrap_features: [False] # [True]
    n_estimators_bag: [10] # [25, 50, 100, 1000, 10000]
    max_features_bag: [2] # [4, 8, 16]
  RandomForestBoosting:
    n_estimators: [100] # [25, 50, 100, 1000, 10000]
    max_depth: [20] # [10, 20, 50, 100]
    max_features: [2] # ['sqrt', 'log2', 2, 4, 8, 16, "auto"]
    criterion: ['gini'] # ['entropy']
    min_samples_split: [2] # [5, 10]
    algorithm: ['SAMME'] # ['SAMME.R']
    learning_rate: [0.01] # [0.1, 1, 10, 100]
    n_estimators_boost: [10] # [25, 50, 100, 1000, 10000]
  ExtraTrees:
    n_estimators: [ 10] # [25, 50, 100, 1000, 10000]
    max_depth: [3 ] #  5, 10] # [20, 50, 100]
    max_features: ['log2'] # [4, 8, 16, "auto"]
    criterion: ['gini'] #, 'entropy']
    min_samples_split: [2] #, 5, 10]
  AdaBoost:
    algorithm: ['SAMME', 'SAMME.R']
    n_estimators: [1, 10, 100]  # [1000, 10000]
    learning_rate: [0.01, 0.1, 1, 10, 100]
  LogisticRegression:
    C_reg: [0.00001, 0.0001, 0.001, 0.01, 0.1]  # [1, 10]
    penalty: ['l1', 'l2']
  SVM:
    C_reg: [0.00001, 0.0001, 0.001, 0.01, 0.1]  # [1, 10]
    kernel: ['linear']
  GradientBoostingClassifier:
    n_estimators: [1, 10, 100]  # [1000, 10000]
    learning_rate: [0.001, 0.01, 0.05, 0.1, 0.5]
    subsample: [0.1, 0.5, 1.0]
    max_depth: [1, 3, 5, 10, 20]  # [50, 100]
  DecisionTreeClassifier:
    criterion: ['gini', 'entropy']
    max_depth: [1, 5, 10, 20]  # [50, 100]
    max_features: ['sqrt', 'log2']
    min_samples_split: [2, 5, 10]
  SGDClassifier:
    loss: ['log', 'modified_huber']
    penalty: ['l1', 'l2', 'elasticnet']
  KNeighborsClassifier:
    n_neighbors: [1, 3, 5, 10, 25, 50, 100]
    weights: ['uniform', 'distance']
    algorithm: ['auto', 'kd_tree']
